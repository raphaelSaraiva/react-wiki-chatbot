{
  "id": "t143",
  "table_no": 143,
  "name": "Intervalos de Tempo para Recompensas",
  "iso25010": {
    "characteristic": "Eficiência de Desempenho",
    "subcharacteristic": "Comportamento Temporal"
  },
  "fields": {
    "definition": "Mede o número de iterações (ou passos temporais) necessários para que o agente de aprendizado por reforço em\num sistema blockchain atinja uma recompensa estável\nou ótima. Em um contexto de blockchain aplicado a\nenergy trading, representa o tempo de aprendizado para\nmaximizar o ganho de recompensas (incentivos, eficiência energética, ou redução de penalidades) em operações\nbaseadas em contratos inteligentes.",
    "objective": "Quantificar a eficiência de convergência de algoritmos de\naprendizado de máquina integrados à blockchain, avaliando a rapidez com que o sistema alcança estabilidade\nnas decisões (por exemplo, seleção de prosumidores para\nnegociação de energia) e reduz o número de interações\nnecessárias para otimização das recompensas.\nModelo\nde\nQualidade\nISO/IEC 25010",
    "equation": "Treward = min(T) sujeito a R(T) ≥Rthreshold\nonde Treward é o número de timesteps necessários até\nque a recompensa acumulada R(T) ultrapasse um limite de estabilidade Rthreshold, indicando convergência\ndo aprendizado.\nMétricas\nAssociadas\nReward Accumulation; Convergence Rate; Learning Efficiency; Penalty Rate\nAtributo\nRelacionado\nTempo de Convergência; Eficiência de Aprendizado; Estabilidade de Política de Decisão\n329",
    "protocol": "1) Definir os parâmetros do ambiente de aprendizado\n(episodes, learning rate, discount factor).\n2) Registrar o valor da recompensa Rt em cada timestep\nt durante a execução do algoritmo Q-learning.\n3) Calcular o número de timesteps necessários até que\na diferença entre Rt+1 e Rt seja inferior ao limiar de\nconvergência ρ.\n4) Repetir o processo para diferentes configurações de\naprendizado e comparar o número médio de timesteps\naté a convergência.\n5) Analisar a correlação entre o número de timesteps e o\ndesempenho energético ou econômico do sistema.",
    "comment": "O número de timesteps para alcançar a recompensa ideal\né um indicador crítico do desempenho do algoritmo de\naprendizado em sistemas descentralizados. No contexto\ndo PRS-P2P, a métrica reflete a rapidez com que o\nagente de aprendizado define políticas de negociação\nde energia equilibradas e justas entre consumidores e\nprosumidores. Menores valores de timesteps indicam\naprendizado mais eficiente e respostas mais rápidas em\nambientes blockchain de alta complexidade.\nInterpretação\ndo\nValor Medido\nValores menores de Treward indicam aprendizado mais\nrápido e eficiente. Valores elevados sugerem instabilidade na política de decisão ou parâmetros inadequa-\ndos de aprendizado (como learning rate ou discount\nfactor).",
    "scale_type": "Razão",
    "references": "Kumari, A.; Gupta, R.; & Tanwar, S. (2021). PRS-P2P: A Prosumer Recommender System for Secure P2P Energy Trading using\nQ-Learning Towards 6G. IEEE International Conference on\nCommunications Workshops (ICC Workshops), 2021. —\nMétrica avaliada para comparar o desempenho do modelo proposto\nem termos de número de timesteps necessários à convergência da\nrecompensa e eficiência de aprendizado.\n331"
  },
  "source": {
    "pdf": "catalogo_de_metricas.pdf",
    "table": "Tabela 143"
  }
}
