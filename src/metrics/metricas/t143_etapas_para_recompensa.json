{
  "id": "t143",
  "table_no": 143,
  "name": "Intervalos de Tempo para Recompensas",
  "iso25010": {
    "characteristic": "Eficiência de Desempenho",
    "subcharacteristic": "Comportamento Temporal"
  },
  "fields": {
    "definition": "Mede o número de iterações (ou passos temporais) necessários para que o agente de aprendizado por reforço em\num sistema blockchain atinja uma recompensa estável\nou ótima. Em um contexto de blockchain aplicado a\nenergy trading, representa o tempo de aprendizado para\nmaximizar o ganho de recompensas (incentivos, eficiência energética, ou redução de penalidades) em operações\nbaseadas em contratos inteligentes.",
    "objective": "Quantificar a eficiência de convergência de algoritmos de\naprendizado de máquina integrados à blockchain, avaliando a rapidez com que o sistema alcança estabilidade\nnas decisões (por exemplo, seleção de prosumidores para\nnegociação de energia) e reduz o número de interações\nnecessárias para otimização das recompensas.\nModelo\nde\nQualidade\nISO/IEC 25010",
    "equation": "Treward = min(T) sujeito a R(T) ≥Rthreshold\nonde Treward é o número de timesteps necessários até\nque a recompensa acumulada R(T) ultrapasse um limite de estabilidade Rthreshold, indicando convergência\ndo aprendizado.\nMétricas\nAssociadas\nReward Accumulation; Convergence Rate; Learning Efficiency; Penalty Rate\nAtributo\nRelacionado\nTempo de Convergência; Eficiência de Aprendizado; Estabilidade de Política de Decisão\n329",
    "protocol": "1) Definir os parâmetros do ambiente de aprendizado\n(episodes, learning rate, discount factor).\n2) Registrar o valor da recompensa Rt em cada timestep\nt durante a execução do algoritmo Q-learning.\n3) Calcular o número de timesteps necessários até que\na diferença entre Rt+1 e Rt seja inferior ao limiar de\nconvergência ρ.\n4) Repetir o processo para diferentes configurações de\naprendizado e comparar o número médio de timesteps\naté a convergência.\n5) Analisar a correlação entre o número de timesteps e o\ndesempenho energético ou econômico do sistema.",
    "comment": "O número de timesteps para alcançar a recompensa ideal\né um indicador crítico do desempenho do algoritmo de\naprendizado em sistemas descentralizados. No contexto\ndo PRS-P2P, a métrica reflete a rapidez com que o\nagente de aprendizado define políticas de negociação\nde energia equilibradas e justas entre consumidores e\nprosumidores. Menores valores de timesteps indicam\naprendizado mais eficiente e respostas mais rápidas em\nambientes blockchain de alta complexidade.\nInterpretação\ndo\nValor Medido\nValores menores de Treward indicam aprendizado mais\nrápido e eficiente. Valores elevados sugerem instabilidade na política de decisão ou parâmetros inadequa-\ndos de aprendizado (como learning rate ou discount\nfactor).",
    "unit": "Número de iterações (steps)",
    "scale_type": "Razão",
    "precision": "Depende da granularidade de registro das iterações e da\nestabilidade da função de recompensa. Requer sincronização temporal precisa entre os ciclos de aprendizado e\no armazenamento dos resultados na blockchain.",
    "data_collection": "Automática / Experimental (logs de aprendizado)",
    "measurement_tool": "Simuladores de aprendizado por reforço (Python RLlib,\nTensorFlow Agents); módulos de registro de treinamento;\ncontratos inteligentes instrumentados para medição de\niterações.",
    "use_processes": "Avaliação de eficiência em algoritmos Q-learning baseados em blockchain; comparação de desempenho de mo-\ndelos de negociação energética; análise de estabilidade\nde políticas inteligentes em redes distribuídas.\n330",
    "beneficiaries": "Pesquisadores de IA aplicada a blockchain; engenheiros de energia inteligente; desenvolvedores de sistemas\nautônomos descentralizados; projetistas de contratos inteligentes.",
    "references": "Kumari, A.; Gupta, R.; & Tanwar, S. (2021). PRS-P2P: A Prosumer Recommender System for Secure P2P Energy Trading using\nQ-Learning Towards 6G. IEEE International Conference on\nCommunications Workshops (ICC Workshops), 2021. —\nMétrica avaliada para comparar o desempenho do modelo proposto\nem termos de número de timesteps necessários à convergência da\nrecompensa e eficiência de aprendizado.\n331"
  },
  "raw_fields": {
    "Atributo Nome (Título)": "Intervalos de Tempo para Recompensas",
    "Palavras-chave / Alias": "Timesteps for rewards; Q-learning episodes; Reward convergence; Reinforcement learning iteration steps; Energy\ntrading learning rate",
    "Definição do Atributo": "Mede o número de iterações (ou passos temporais) necessários para que o agente de aprendizado por reforço em\num sistema blockchain atinja uma recompensa estável\nou ótima. Em um contexto de blockchain aplicado a\nenergy trading, representa o tempo de aprendizado para\nmaximizar o ganho de recompensas (incentivos, eficiência energética, ou redução de penalidades) em operações\nbaseadas em contratos inteligentes.",
    "Objetivo / Motivação": "Quantificar a eficiência de convergência de algoritmos de\naprendizado de máquina integrados à blockchain, avaliando a rapidez com que o sistema alcança estabilidade\nnas decisões (por exemplo, seleção de prosumidores para\nnegociação de energia) e reduz o número de interações\nnecessárias para otimização das recompensas.\nModelo\nde\nQualidade\nISO/IEC 25010",
    "Característica": "Eficiência de Desempenho",
    "Sub-característica": "Comportamento Temporal",
    "Equação": "Treward = min(T) sujeito a R(T) ≥Rthreshold\nonde Treward é o número de timesteps necessários até\nque a recompensa acumulada R(T) ultrapasse um limite de estabilidade Rthreshold, indicando convergência\ndo aprendizado.\nMétricas\nAssociadas\nReward Accumulation; Convergence Rate; Learning Efficiency; Penalty Rate\nAtributo\nRelacionado\nTempo de Convergência; Eficiência de Aprendizado; Estabilidade de Política de Decisão\n329",
    "Protocolo": "1) Definir os parâmetros do ambiente de aprendizado\n(episodes, learning rate, discount factor).\n2) Registrar o valor da recompensa Rt em cada timestep\nt durante a execução do algoritmo Q-learning.\n3) Calcular o número de timesteps necessários até que\na diferença entre Rt+1 e Rt seja inferior ao limiar de\nconvergência ρ.\n4) Repetir o processo para diferentes configurações de\naprendizado e comparar o número médio de timesteps\naté a convergência.\n5) Analisar a correlação entre o número de timesteps e o\ndesempenho energético ou econômico do sistema.",
    "Comentário": "O número de timesteps para alcançar a recompensa ideal\né um indicador crítico do desempenho do algoritmo de\naprendizado em sistemas descentralizados. No contexto\ndo PRS-P2P, a métrica reflete a rapidez com que o\nagente de aprendizado define políticas de negociação\nde energia equilibradas e justas entre consumidores e\nprosumidores. Menores valores de timesteps indicam\naprendizado mais eficiente e respostas mais rápidas em\nambientes blockchain de alta complexidade.\nInterpretação\ndo\nValor Medido\nValores menores de Treward indicam aprendizado mais\nrápido e eficiente. Valores elevados sugerem instabilidade na política de decisão ou parâmetros inadequa-\ndos de aprendizado (como learning rate ou discount\nfactor).",
    "Unidade": "Número de iterações (steps)",
    "Tipo de Escala": "Razão",
    "Precisão": "Depende da granularidade de registro das iterações e da\nestabilidade da função de recompensa. Requer sincronização temporal precisa entre os ciclos de aprendizado e\no armazenamento dos resultados na blockchain.",
    "Tipo de Coleta de Dados": "Automática / Experimental (logs de aprendizado)",
    "Ferramenta de Medição": "Simuladores de aprendizado por reforço (Python RLlib,\nTensorFlow Agents); módulos de registro de treinamento;\ncontratos inteligentes instrumentados para medição de\niterações.",
    "Processos de Uso Potenciais": "Avaliação de eficiência em algoritmos Q-learning baseados em blockchain; comparação de desempenho de mo-\ndelos de negociação energética; análise de estabilidade\nde políticas inteligentes em redes distribuídas.\n330",
    "Beneficiários Potenciais": "Pesquisadores de IA aplicada a blockchain; engenheiros de energia inteligente; desenvolvedores de sistemas\nautônomos descentralizados; projetistas de contratos inteligentes.",
    "Referências": "Kumari, A.; Gupta, R.; & Tanwar, S. (2021). PRS-P2P: A Prosumer Recommender System for Secure P2P Energy Trading using\nQ-Learning Towards 6G. IEEE International Conference on\nCommunications Workshops (ICC Workshops), 2021. —\nMétrica avaliada para comparar o desempenho do modelo proposto\nem termos de número de timesteps necessários à convergência da\nrecompensa e eficiência de aprendizado.\n331"
  },
  "source": {
    "pdf": "catalogo_de_metricas.pdf",
    "table": "Tabela 143"
  }
}
